{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4596858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:03:51.596857Z",
     "start_time": "2025-11-11T17:03:51.588060Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.signal import find_peaks\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ff32d",
   "metadata": {},
   "source": [
    "#### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafcd3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:03:43.016324Z",
     "start_time": "2025-11-11T16:53:22.587539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pfad zur Excel-Datei\n",
    "excel_path = \"../data/gebundelte_eco_counter_fahrradzaehler_bw.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# Zeilen filtern, deren 'data_description' \"CSV\" enthält\n",
    "filtered_df = df[df['data_description'].str.contains(\"CSV\", case=False, na=False)]\n",
    "\n",
    "# URLs extrahieren\n",
    "url_list = filtered_df['data_download_url'].dropna().tolist()\n",
    "\n",
    "# Ausgabe (optional)\n",
    "#for url in url_list:\n",
    "#    print(url)\n",
    "\n",
    "general_columns = pd.read_csv(url_list[1]).columns.tolist()\n",
    "\n",
    "# Erstelle CSV-Datei, in der Daten aller URLs gespeichert werden\n",
    "all_data = pd.DataFrame()\n",
    "for url in url_list:\n",
    "    csv_data = pd.read_csv(url)\n",
    "    assert list(csv_data.columns) == general_columns, f\"Spalten stimmen nicht überein in {url}\"\n",
    "    all_data = pd.concat([all_data, csv_data], ignore_index=True)\n",
    "\n",
    "# Speichere all_data lokal als CSV-Datei\n",
    "all_data.to_csv(\"alle_fahrradzaehler_daten.csv\", index=False)   \n",
    "all_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fbd19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:06:07.899386Z",
     "start_time": "2025-11-11T17:04:38.936348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Einlesen von alle_fahrradzaehler_daten.csv\n",
    "data = pd.read_csv(\"../alle_fahrradzaehler_daten.csv\")\n",
    "data['iso_timestamp'] = pd.to_datetime(data['iso_timestamp'], utc = True, errors='coerce') # Isotimestamp is lokale Zeit und berücksichtigt Sommerzeit\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data \n",
    "print(data.info())\n",
    "#print(data.describe())\n",
    "#print(data.isnull().sum())\n",
    "#print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f51a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = data.duplicated()\n",
    "print(f\"Anzahl der Duplikate: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dc21b",
   "metadata": {},
   "source": [
    "#### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of years in the dataset\n",
    "data['year'] = data['timestamp'].dt.year\n",
    "print(f\"Jahre im Datensatz: {data['year'].nunique()} - {data['year'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations in data set\n",
    "locations = data['standort'].unique()\n",
    "print(f\"Anzahl der Standorte: {len(locations)}\")\n",
    "print(f\"Standorte:\")\n",
    "for loc in locations:\n",
    "    print(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cc554",
   "metadata": {},
   "source": [
    "Note: counter = Gerät, channel = Richtung/Messspur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3356a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the amount of counters per location\n",
    "counters_per_location = data.groupby('standort').agg(\n",
    "    anzahl_counter=('counter_serial', 'nunique'),\n",
    "    counter_sites=('counter_site', lambda x: ', '.join(sorted(set(x))))\n",
    ").reset_index()\n",
    "\n",
    "print(\"Anzahl der Zähler pro Standort:\") \n",
    "counters_per_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_per_location = data.groupby('standort')['counter_site_id'].nunique()\n",
    "counters_per_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check, at which location (standort) counter_serial is null\n",
    "null_serial_locations = data[data['counter_serial'].isnull()]['standort'].unique()\n",
    "print(f\"Standorte mit null counter_serial: {null_serial_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many channels there are for a counter side\n",
    "channels_per_site = (\n",
    "    data.groupby(['standort', 'counter_site_id'])['channel_name']\n",
    "        .nunique()\n",
    "        .reset_index(name='num_channels')\n",
    ")\n",
    "\n",
    "# Kurze Übersicht\n",
    "print(\"Beschreibung der Anzahl Channels pro Counter-Side:\")\n",
    "print(channels_per_site['num_channels'].describe())\n",
    "print(\"\\nBeispiele:\")\n",
    "print(channels_per_site.head())\n",
    "\n",
    "# Histogramm\n",
    "plt.figure(figsize=(8,4))\n",
    "max_bins = channels_per_site['num_channels'].max()\n",
    "plt.hist(channels_per_site['num_channels'], bins=range(1, max_bins+2), align='left', edgecolor='black')\n",
    "plt.xlabel('Anzahl Channels pro Counter')\n",
    "plt.ylabel('Anzahl Counter')\n",
    "plt.title('Verteilung: Channels pro Counter')\n",
    "plt.xticks(range(1, max_bins+1))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b86ad",
   "metadata": {},
   "source": [
    "#### Check Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counter per city\n",
    "city = \"Stadt Heilbronn\"\n",
    "data_city = data[data['standort'] == city]\n",
    "print(data_city[\"domain_id\"].unique()[0])\n",
    "counters = data_city[['counter_site', 'counter_site_id', 'counter_serial']].drop_duplicates().reset_index(drop=True)\n",
    "tracking = data_city.groupby(['counter_site', 'counter_site_id', 'counter_serial'], dropna = False)['iso_timestamp'] \\\n",
    "    .agg(first_timestamp='min', last_timestamp='max') \\\n",
    "    .reset_index()\n",
    "counters_with_tracking = counters.merge(tracking, on=['counter_site', 'counter_site_id', 'counter_serial'])\n",
    "counters_with_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4eebd8",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a231d",
   "metadata": {},
   "source": [
    "##### Number counts per Location per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afda89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check yearly number of counts per standort \n",
    "\n",
    "# Extract date from timestamp\n",
    "data['date'] = data['timestamp'].dt.date\n",
    "\n",
    "# Step 1: Get max zählstand per standort, counter_serial, and date    #TODO: warum das Maximum extrahieren statt einfach alle zählstände zusammenzuaddieren?\n",
    "max_per_counter = data.groupby(['standort', 'counter_serial', 'date'])['zählstand'].max().reset_index() #TODO: lieber counter_side_id nutzen?\n",
    "\n",
    "# Step 2: Sum these maxima per standort and date\n",
    "daily_counts = max_per_counter.groupby(['standort', 'date'])['zählstand'].sum().reset_index()\n",
    "daily_counts = daily_counts.rename(columns={'zählstand': 'daily_counts'})\n",
    "\n",
    "# Step 3: Now aggregate to yearly counts\n",
    "daily_counts['year'] = pd.to_datetime(daily_counts['date']).dt.year\n",
    "yearly_counts = daily_counts.groupby(['standort', 'year'])['daily_counts'].sum().reset_index()\n",
    "yearly_counts = yearly_counts.rename(columns={'daily_counts': 'yearly_counts'})\n",
    "\n",
    "print(\"Jährliche Zählungen pro Standort:\")\n",
    "yearly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093837be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the yearly counts per standort #TODO: sind das nicht eher die Tages-Maximas aufsummiert und nicht die tatsächlichen counts?\n",
    "def thousands(x, pos):\n",
    "    return f'{int(x/1000)}'\n",
    "\n",
    "standorte = yearly_counts['standort'].unique()\n",
    "colors = cm.get_cmap('tab20', len(standorte))  \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, standort in enumerate(standorte):\n",
    "    subset = yearly_counts[yearly_counts['standort'] == standort]\n",
    "    plt.plot(subset['year'], subset['yearly_counts'], marker='o', \n",
    "             label=standort, color=colors(i))\n",
    "\n",
    "plt.title(\"Jährliche Fahrradzählungen pro Standort\")\n",
    "plt.xlabel(\"Jahr\")\n",
    "plt.ylabel(\"Anzahl der Fahrradzählungen (in Tsd.)\")\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e52a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Konstanz, check how many counters there are per year\n",
    "konstanz_data = data[data['standort'] == 'Stadt Konstanz']\n",
    "counters_per_year_konstanz = konstanz_data.groupby('year')['counter_serial'].nunique()\n",
    "counters_per_year_konstanz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of counters per year in Konstanz\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(counters_per_year_konstanz.index, counters_per_year_konstanz.values, marker='o')\n",
    "plt.title(\"Anzahl der Zähler pro Jahr in Konstanz\")\n",
    "plt.xlabel(\"Jahr\")\n",
    "plt.ylabel(\"Anzahl der Zähler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc65f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize counts per location by number of counters\n",
    "\n",
    "# First, calculate number of active counters per standort and date i.e. counters that have non-null zählstand on that date as max value\n",
    "active_counters = data.groupby(['standort', 'counter_serial', 'date'])['zählstand'].max().reset_index()\n",
    "active_counters = active_counters[active_counters['zählstand'].notnull()]\n",
    "active_counters = active_counters.groupby(['standort', 'date'])['counter_serial'].nunique().reset_index()\n",
    "active_counters = active_counters.rename(columns={'counter_serial': 'num_active_counters'})\n",
    "\n",
    "# Merge mit daily_counts\n",
    "daily_counts = daily_counts.merge(active_counters, on=['standort', 'date'], how='left')\n",
    "\n",
    "# Normalize daily counts\n",
    "daily_counts['normalized_daily_counts'] = daily_counts['daily_counts'] / daily_counts['num_active_counters']\n",
    "\n",
    "\n",
    "# Aggregate to yearly normalized counts\n",
    "normalized_yearly_counts = daily_counts.groupby(['standort', 'year'])['normalized_daily_counts'].sum().reset_index()\n",
    "normalized_yearly_counts = normalized_yearly_counts.rename(columns={'normalized_daily_counts': 'normalized_yearly_counts'})\n",
    "\n",
    "print(\"Normalisierte jährliche Zählungen pro Standort:\")\n",
    "normalized_yearly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized yearly counts per standort\n",
    "def thousands(x, pos):\n",
    "    return f'{int(x/1000)}'\n",
    "\n",
    "standorte = normalized_yearly_counts['standort'].unique()\n",
    "colors = cm.get_cmap('tab20', len(standorte))  # Colormap\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, standort in enumerate(standorte):\n",
    "    subset = normalized_yearly_counts[normalized_yearly_counts['standort'] == standort]\n",
    "    plt.plot(subset['year'], subset['normalized_yearly_counts'], marker='o',  # hier die Jahreswerte verwenden!\n",
    "             label=standort, color=colors(i))\n",
    "\n",
    "plt.title(\"Jährliche Fahrradzählungen pro Standort (normalisiert)\")\n",
    "plt.xlabel(\"Jahr\")\n",
    "plt.ylabel(\"Anzahl der Fahrradzählungen/# Aktive Tracker (in Tsd.)\")\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printe für jeden Standort die Anzahl der Datenpunkte\n",
    "for location in data['standort'].unique():\n",
    "    count = data[(data['standort'] == location)].shape[0]\n",
    "    if count > 0:\n",
    "        print(f\"Standort: {location}, Anzahl der Datenpunkte: {count}\")\n",
    "\n",
    "#printe für jeden Standort alle Channel-Namen:\n",
    "for location in data['standort'].unique():\n",
    "    channel_names = data[data['standort'] == location]['channel_name'].unique()\n",
    "    print(f\"Standort: {location}, Channel Names: {channel_names}\")\n",
    "\n",
    "# printe für jeden Channel Name am Standort Stadt Freiburg die Anzahl der Datenpunkte:\n",
    "for channel_name in data['channel_name'].unique():\n",
    "    count = data[(data['standort'] == \"Stadt Freiburg\") & (data['channel_name'] == channel_name)].shape[0]\n",
    "    if count > 0:\n",
    "        print(f\"Standort: Stadt Freiburg, Channel Name: {channel_name}, Anzahl der Datenpunkte: {count}\")\n",
    "\n",
    "\"\"\"for location in data['standort'].unique():\n",
    "    for channel_name in data['channel_name'].unique():\n",
    "        count = data[(data['standort'] == location) & (data['channel_name'] == channel_name)].shape[0]\n",
    "        if count > 0:\n",
    "            print(f\"Standort: {location}, Channel Name: {channel_name}, Anzahl der Datenpunkte: {count}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30466f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prüfe, für jeden Standort, wie viele Datenpunkte es gibt \n",
    "#for location in data['standort'].unique():\n",
    "location = \"Stadt Freiburg\"\n",
    "for site_id in data['counter_site_id'].unique():\n",
    "    count = data[(data['standort'] == location) & (data['counter_site_id'] == site_id)].shape[0]\n",
    "    if count > 0:\n",
    "        print(f\"Standort: {location}, Counter Side ID: {site_id}, Anzahl der Datenpunkte: {count}\")\n",
    "\n",
    "for channel_name in data['channel_name'].unique():\n",
    "    count = data[(data['standort'] == location) & (data['channel_name'] == channel_name)].shape[0]\n",
    "    if count > 0:\n",
    "        print(f\"Standort: {location}, Channel Name: {channel_name}, Anzahl der Datenpunkte: {count}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printe für jeden Standort, Channel Name und Counter Site ID die Anzahl der Datenpunkte\n",
    "for location in data['standort'].unique():\n",
    "    data_standort = data[data['standort'] == location]\n",
    "    for site_id in data_standort['counter_site_id'].unique():\n",
    "        for channel_name in data_standort['channel_name'].unique():\n",
    "            count = data[(data['standort'] == location) & (data['counter_site_id'] == site_id) & (data['channel_name'] == channel_name)].shape[0]\n",
    "            if count > 0:\n",
    "                print(f\"Standort: {location}, Channel Name: {channel_name}, Counter Side ID: {site_id}, Anzahl der Datenpunkte: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# prüfe für jeden Counter (Kombination aus Standort, Channel_name und Counter_Site_ID), \n",
    "# was der jeweils früheste und späteste timestamp ist\n",
    "# schreibe standort, channel_name, frühester und spätester timestamp in pandas dataframe\n",
    "start_end_timestamps = pd.DataFrame(columns=['standort', 'channel_name', 'counter_site_id', 'earliest_timestamp', 'latest_timestamp'])\n",
    "\n",
    "for location in data['standort'].unique():\n",
    "    data_standort = data[data['standort'] == location]\n",
    "    for site_id in data_standort['counter_site_id'].unique():\n",
    "        for channel_name in data_standort['channel_name'].unique():\n",
    "            subset = data[(data['standort'] == location) & (data['counter_site_id'] == site_id) & (data['channel_name'] == channel_name)]\n",
    "            if not subset.empty:\n",
    "                earliest = subset['timestamp'].min().replace(tzinfo=None)\n",
    "                latest = subset['timestamp'].max().replace(tzinfo=None)\n",
    "\n",
    "                start_end_timestamps = pd.concat([start_end_timestamps, pd.DataFrame([{\n",
    "                    'standort': location,\n",
    "                    'channel_name': channel_name,\n",
    "                    'counter_site_id': site_id,\n",
    "                    'earliest_timestamp': earliest,\n",
    "                    'latest_timestamp': latest\n",
    "                }])], ignore_index=True)                       \n",
    "                \n",
    "    print(location)\n",
    "                \n",
    "    \"\"\"if earliest > datetime(2013, 1, 1, 1, 0, 0) or latest < datetime(2024, 12, 31, 23, 0, 0):\n",
    "    #datetime(2023, 11, 7, 14, 30, 0)  # Jahr, Monat, Tag, Stunde, Minute, Sekunde\n",
    "        print(f\"{location}, {channel_name}, Frühester Timestamp: {earliest}, Spätester Timestamp: {latest}\")\n",
    "    else:\n",
    "        print(f\"{location}, {channel_name}, passt zeitlich\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe start_end_timestamps in csv-datei\n",
    "start_end_timestamps.to_csv(\"start_end_timestamps.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_literacy_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
