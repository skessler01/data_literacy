{
 "cells": [
  {
   "cell_type": "code",
   "id": "0060b70e",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import requests_cache\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.preprocessing_utils import detect_intervals_with_missing_data, interpolate_short_gaps, remove_long_zero_intervals, split_long_gaps, cap_outliers_by_time_pattern\n",
    "from utils.mstl_utils import process_city_mstl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04e36037",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec47c5",
   "metadata": {},
   "source": [
    "Bike data"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6511987",
   "metadata": {},
   "source": [
    "## Read in .csv file of all bike data if it exists, otherwise download data from URLs and save as .csv\n",
    "\n",
    "file_path = \"../data/full_bike_data.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    #print(f\"Datei existiert: {file_path}\")\n",
    "    bike_data_raw = pd.read_csv(\"../data/full_bike_data.csv\", low_memory=False)\n",
    "    \n",
    "else:\n",
    "    #print(f\"Datei fehlt: {file_path}\")\n",
    "    url_list = []\n",
    "\n",
    "    # URLs zu Daten generieren nach folgendem Schema:\n",
    "    # https://mobidata-bw.de/daten/eco-counter/v2/fahrradzaehler_stundenwerten_{yyyymm}.csv.gz \n",
    "\n",
    "    for year in range(2013, 2026):\n",
    "        for month in range(1, 13):\n",
    "            yyyymm = f\"{year}{month:02d}\"\n",
    "            url = f\"https://mobidata-bw.de/daten/eco-counter/v2/fahrradzaehler_stundenwerten_{yyyymm}.csv.gz\"\n",
    "            \n",
    "            # Überprüfen, ob die URL existiert\n",
    "            response = requests_cache.CachedSession().head(url)\n",
    "            if response.status_code == 200:    \n",
    "                # in url_list hinzufügen\n",
    "                url_list.append(url)\n",
    "\n",
    "    general_columns = pd.read_csv(url_list[1]).columns.tolist()\n",
    "\n",
    "    # Erstelle CSV-Datei, in der Daten aller URLs gespeichert werden\n",
    "    full_bike_data = pd.DataFrame()\n",
    "    for url in url_list:\n",
    "        csv_data = pd.read_csv(url, low_memory=False)\n",
    "        assert list(csv_data.columns) == general_columns, f\"Spalten stimmen nicht überein in {url}\"\n",
    "        full_bike_data = pd.concat([full_bike_data, csv_data], ignore_index=True)\n",
    "\n",
    "    # Speichere full_bike_data lokal als CSV-Datei\n",
    "    full_bike_data.to_csv(\"../data/full_bike_data.csv\", index=False)   \n",
    "    bike_data_raw = full_bike_data.copy()\n",
    "\n",
    "bike_data_raw['iso_timestamp'] = pd.to_datetime(bike_data_raw['iso_timestamp'], utc = True, errors='coerce') # Isotimestamp ist lokale Zeit und berücksichtigt Sommerzeit\n",
    "bike_data_raw.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0c20ccd",
   "metadata": {},
   "source": [
    "bike_data_raw.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13319ae6",
   "metadata": {},
   "source": [
    "# Übersicht über die Counters in einer Stadt\n",
    "city = \"Stadt Heidelberg\"\n",
    "data_city = bike_data_raw[bike_data_raw['domain_name'] == city]\n",
    "\n",
    "counters = data_city[['counter_site', 'counter_site_id', 'counter_serial']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tracking = data_city.groupby(['counter_site', 'counter_site_id', 'counter_serial'], dropna = False)['iso_timestamp'] \\\n",
    "    .agg(first_timestamp='min', last_timestamp='max') \\\n",
    "    .reset_index()\n",
    "counters_with_tracking = counters.merge(tracking, on=['counter_site', 'counter_site_id', 'counter_serial'])\n",
    "\n",
    "counters_with_tracking"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e312946",
   "metadata": {},
   "source": [
    "Weather data"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa50c4b9",
   "metadata": {},
   "source": [
    "## Read in csv file\n",
    "#weather_data = pd.read_csv(\"../data/weather_per_city.csv\")\n",
    "#weather_data['timestamp'] = pd.to_datetime(weather_data['date'], utc = True, errors='coerce') # Timestamp is in UTC\n",
    "#weather_data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "349f7d33",
   "metadata": {},
   "source": [
    "#weather_data.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1141649",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb956ea2",
   "metadata": {},
   "source": [
    "##### Clean Bike Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e11e17e",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "data_cleaned = bike_data_raw.copy()\n",
    "\n",
    "# Keep only relevant columns\n",
    "data_cleaned = data_cleaned[['domain_name', 'counter_site', 'longitude', 'latitude',\n",
    "       'iso_timestamp', 'channels_all']]\n",
    "\n",
    "# Only keep the following cities: Freiburg, Tübingen, Stuttgart, Ludwigsburg, Mannheim, Heidelberg, Reutlingen\n",
    "cities_to_keep = [\"Stadt Freiburg\", \"Stadt Tübingen\", \"Landeshauptstadt Stuttgart\", \n",
    "                  \"Stadt Ludwigsburg\", \"Stadt Mannheim\", \"Stadt Heidelberg\", \"Stadt Reutlingen\"]\n",
    "data_cleaned = data_cleaned[data_cleaned['domain_name'].isin(cities_to_keep)].copy()\n",
    "\n",
    "# 'Isotimestamp' is local time and considers 'Sommerzeit'. Therefore, we use this for better accuracy in time representation.\n",
    "# Exchange 'timestamp' with 'iso_timestamp' and convert to datetime with UTC timezone.\n",
    "# Drop 'timezone' as this is identical for all entries.\n",
    "data_cleaned['timestamp'] = pd.to_datetime(data_cleaned['iso_timestamp'], utc = True, errors='coerce') \n",
    "data_cleaned['timestamp'] = data_cleaned['timestamp'].dt.tz_convert('Europe/Berlin')\n",
    "data_cleaned = data_cleaned.drop(columns=['iso_timestamp'])\n",
    "\n",
    "# Rename columns for better clarity\n",
    "data_cleaned = data_cleaned.rename(columns={'domain_name': 'city', 'channels_all': 'count'})\n",
    "\n",
    "# Save cleaned data\n",
    "data_cleaned.to_csv(\"../data/full_bike_data_cleaned.csv\", index=False)\n",
    "\n",
    "bike_data = data_cleaned.copy()\n",
    "bike_data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cb08f33",
   "metadata": {},
   "source": [
    "bike_data.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2dac002e",
   "metadata": {},
   "source": [
    "Sanity Check: Do channels_in & channels_out & channel_unknown sum up to channels out? # TODO: Add other sanity checks we did like checking that # counter_name = # counter ids etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "4222bfda",
   "metadata": {},
   "source": [
    "\n",
    "data_checksum = bike_data_raw.copy()\n",
    "# ersetze 'na' Werte in den 3 channel spalten durch 0, um Checksum berechnen zu können\n",
    "data_checksum['channels_in'] = np.where(data_checksum['channels_in'].eq('na'), 0, data_checksum['channels_in'])\n",
    "data_checksum['channels_out'] = np.where(data_checksum['channels_out'].eq('na'), int(0), data_checksum['channels_out'])\n",
    "data_checksum['channels_unknown'] = np.where(data_checksum['channels_unknown'].eq('na'), int(0), data_checksum['channels_unknown'])\n",
    "\n",
    "# konvertiere die 4 Spalten in Integer\n",
    "data_checksum[['channels_in', 'channels_out', 'channels_unknown', 'count']] = data_checksum[['channels_in', 'channels_out', 'channels_unknown', 'count']].astype(int)\n",
    "sum_cols = ['channels_in', 'channels_out', 'channels_unknown']\n",
    "\n",
    "# Prüfe, ob die Summe der 3 channel Spalten der count Spalte entspricht\n",
    "data_checksum['checksum_correct'] = data_checksum[sum_cols].sum(axis=1).eq(data_checksum['count'])\n",
    "print(data_checksum['checksum_correct'].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c408e6bf",
   "metadata": {},
   "source": [
    "##### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "id": "24418699",
   "metadata": {},
   "source": [
    "# Read in csv. file with full data\n",
    "bike_data = pd.read_csv(\"../data/full_bike_data_cleaned.csv\", low_memory=False)\n",
    "bike_data['timestamp'] = pd.to_datetime(bike_data['timestamp'], utc=True, errors='coerce')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2059d7e2",
   "metadata": {},
   "source": [
    "# Cap high counter values based on hour-of-day and day-of-year patterns\n",
    "cap_const = 10\n",
    "bike_data_capped = cap_outliers_by_time_pattern(bike_data, \"count\", cap_const=cap_const)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "136bf509",
   "metadata": {},
   "source": [
    "##### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "4184ca8f",
   "metadata": {},
   "source": [
    "# Check for NaNs in 'count' column\n",
    "nan_counts = bike_data_capped['count'].isna().sum()\n",
    "print(f'Found {nan_counts} NaN values in count column.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d5ec101",
   "metadata": {},
   "source": [
    "# Repress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LONG_ZERO_LIMIT = 168      # 1 week\n",
    "INTERPOLATION_LIMIT = 3   # hours\n",
    "MIN_TS = 2 * ((365 * 24) +6)     # Minimum years of data required per counter (2 years)\n",
    "\n",
    "all_counters_processed = []\n",
    "summary_list = []\n",
    "\n",
    "for city in bike_data_capped['city'].unique():\n",
    "    print(f\"\\nProcessing city: {city}\\n{'='*80}\")\n",
    "    df_city = bike_data_capped[bike_data_capped['city'] == city]\n",
    "\n",
    "    for counter in df_city['counter_site'].unique():\n",
    "        print(f\"Processing counter: {counter}\")\n",
    "\n",
    "        df_counter = df_city[\n",
    "            df_city['counter_site'] == counter\n",
    "        ].copy()\n",
    "\n",
    "        df_counter['timestamp'] = pd.to_datetime(\n",
    "            df_counter['timestamp'], utc=True\n",
    "        )\n",
    "        df_counter['count'] = pd.to_numeric(\n",
    "            df_counter['count'], errors='coerce'\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Remove long zero-count intervals\n",
    "        # -------------------------------------------------\n",
    "        df_counter, removed_count = remove_long_zero_intervals(\n",
    "            df_counter, LONG_ZERO_LIMIT\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Reindex to hourly frequency\n",
    "        # -------------------------------------------------\n",
    "        df_counter = df_counter.set_index('timestamp').sort_index()\n",
    "\n",
    "        full_index = pd.date_range(\n",
    "            df_counter.index.min(),\n",
    "            df_counter.index.max(),\n",
    "            freq='H',\n",
    "            tz='UTC'\n",
    "        )\n",
    "\n",
    "        df_counter = df_counter.reindex(full_index)\n",
    "        df_counter.index.name = 'timestamp'\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Detect missing intervals\n",
    "        # -------------------------------------------------\n",
    "        missing_intervals = detect_intervals_with_missing_data(\n",
    "            df_counter,\n",
    "            column='count',\n",
    "            mode='missing'\n",
    "        )\n",
    "\n",
    "        total_missing = missing_intervals['n_points'].sum()\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Interpolate short gaps\n",
    "        # -------------------------------------------------\n",
    "        df_counter, interpolated_count = interpolate_short_gaps(\n",
    "            df_counter,\n",
    "            missing_intervals,\n",
    "            INTERPOLATION_LIMIT\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Fill metadata\n",
    "        # -------------------------------------------------\n",
    "        meta_cols = ['city', 'counter_site', 'longitude', 'latitude']\n",
    "        df_counter[meta_cols] = df_counter[meta_cols].ffill().bfill()\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Split long gaps\n",
    "        # -------------------------------------------------\n",
    "        long_gaps = missing_intervals[\n",
    "            missing_intervals['n_points'] > INTERPOLATION_LIMIT\n",
    "        ]\n",
    "\n",
    "        df_counter = split_long_gaps(df_counter, long_gaps)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Final validation + collection\n",
    "        # -------------------------------------------------\n",
    "        for site, g in df_counter.groupby('counter_site'):\n",
    "            if g['count'].isna().any():\n",
    "                raise ValueError(\n",
    "                    f\"NaNs remain after processing counter {site}\"\n",
    "                )\n",
    "\n",
    "            all_counters_processed.append(g)\n",
    "            summary_list.append({\n",
    "                'city': city,\n",
    "                'counter_site': site,\n",
    "                'total_missing': total_missing,\n",
    "                'interpolated': interpolated_count,\n",
    "                'removed zeros': removed_count\n",
    "            })\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Final outputs\n",
    "# =====================================================\n",
    "\n",
    "bike_data_final = (\n",
    "    pd.concat(all_counters_processed)\n",
    "      .sort_index()\n",
    "      .reset_index()\n",
    "      .rename(columns={'index': 'timestamp'})\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# =====================================================\n",
    "# Remove counters with less than 2 years of data\n",
    "# =====================================================\n",
    "counter_data_counts = bike_data_final.groupby('counter_site')['count'].count()\n",
    "counters_to_keep = counter_data_counts[counter_data_counts >= MIN_TS].index\n",
    "bike_data_final = bike_data_final[bike_data_final['counter_site'].isin(counters_to_keep)]\n",
    "\n",
    "summary_df = summary_df[summary_df['counter_site'].isin(counters_to_keep)]\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(f\"Final dataset shape: {bike_data_final.shape}\")\n",
    "print(f\"\\nCounters remaining: {bike_data_final['counter_site'].nunique()}\")\n",
    "print(\"\\nSummary:\")\n",
    "summary_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79a51926",
   "metadata": {},
   "source": [
    "# Save the final preprosessed data\n",
    "bike_data_final.to_csv(\"../data/full_bike_data_preprocessed.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MSTL Decomposition",
   "id": "11b0ddd9c39127a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "d = pd.read_csv(\"../data/full_bike_data_preprocessed.csv\", low_memory=False)\n",
    "for city, df_city in d.groupby(\"city\"):\n",
    "    print(\"Processing city: \", city)\n",
    "    process_city_mstl(df_city, city)\n",
    "    print(\"Finished city\", city)\n"
   ],
   "id": "ab931334f1d3723e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93b2253b",
   "metadata": {},
   "source": [
    "##### plots- kann man wieder löschen"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6a77f30",
   "metadata": {},
   "source": [
    "# bike_data_final einzelne Counter einer Stadt plotten   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "city = [\"Stadt Tübingen\", \"Stadt Freiburg\", \"Landeshauptstadt Stuttgart\"]\n",
    "\n",
    "# Filter für die ausgewählte Stadt\n",
    "city_data = bike_data_final[bike_data_final['city'].isin(city)].copy()\n",
    "\n",
    "# Konvertiere timestamp zu datetime falls nötig\n",
    "city_data['timestamp'] = pd.to_datetime(city_data['timestamp'])\n",
    "\n",
    "# Alle einzigartigen Counter\n",
    "counters = city_data['counter_site'].unique()\n",
    "n_counters = len(counters)\n",
    "\n",
    "# Erstelle Subplots\n",
    "n_cols = 1\n",
    "n_rows = n_counters\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten() if n_counters > 1 else [axes]\n",
    "\n",
    "# Plotte jeden Counter\n",
    "for i, counter in enumerate(counters):\n",
    "    counter_data = city_data[city_data['counter_site'] == counter]\n",
    "    \n",
    "    axes[i].plot(counter_data['timestamp'], counter_data['count'], linewidth=0.5)\n",
    "    axes[i].set_title(counter, fontsize=10)\n",
    "    axes[i].set_xlabel('Zeit')\n",
    "    axes[i].set_ylabel('Anzahl Fahrräder')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c623b11",
   "metadata": {},
   "source": [
    "# plot capped data: for all counters of one city. \n",
    "# Mark the capped values in green. and the old the original value of the capped valuen in red\n",
    "\n",
    "# Filter für die ausgewählte Stadt\n",
    "city_data_original = bike_data[bike_data['city'].isin(city)].copy()\n",
    "city_data_capped = bike_data_capped[bike_data_capped['city'].isin(city)].copy()\n",
    "\n",
    "# Konvertiere timestamps zu datetime\n",
    "city_data_original['timestamp'] = pd.to_datetime(city_data_original['timestamp'])\n",
    "city_data_capped['timestamp'] = pd.to_datetime(city_data_capped['timestamp'])\n",
    "\n",
    "# Merge original und capped für Vergleich\n",
    "merged = city_data_original.merge(\n",
    "    city_data_capped[['timestamp', 'counter_site', 'count']], \n",
    "    on=['timestamp', 'counter_site'], \n",
    "    suffixes=('_original', '_capped'),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Alle einzigartigen Counter\n",
    "counters = merged['counter_site'].unique()\n",
    "n_counters = len(counters)\n",
    "\n",
    "# Erstelle Subplots\n",
    "n_cols = 1\n",
    "n_rows = n_counters\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten() if n_counters > 1 else [axes]\n",
    "\n",
    "# Plotte jeden Counter\n",
    "for i, counter in enumerate(counters):\n",
    "    counter_merged = merged[merged['counter_site'] == counter].sort_values('timestamp')\n",
    "    \n",
    "    # Plotte Original-Linie in blau\n",
    "    axes[i].plot(counter_merged['timestamp'], counter_merged['count_original'], \n",
    "                 linewidth=0.8, color='blue', alpha=0.8, label='Original')\n",
    "    \n",
    "    # Plotte gecappte Linie in schwarz\n",
    "    axes[i].plot(counter_merged['timestamp'], counter_merged['count_capped'], \n",
    "                 linewidth=0.8, color='black', alpha=0.6, label='Gecappt')\n",
    "    \n",
    "    # Markiere Punkte wo gecappt wurde (original > capped)\n",
    "    capped_mask = counter_merged['count_original'] > counter_merged['count_capped']\n",
    "    capped_points = counter_merged[capped_mask]\n",
    "    \n",
    "    # Rote Punkte für Original-Werte, die gecappt wurden\n",
    "    axes[i].scatter(capped_points['timestamp'], capped_points['count_original'], \n",
    "                   color='red', s=20, alpha=0.9, label='Original (gecappt)', zorder=5)\n",
    "    \n",
    "    # Grüne Punkte für die gecappten Werte\n",
    "    axes[i].scatter(capped_points['timestamp'], capped_points['count_capped'], \n",
    "                   color='green', s=20, alpha=0.9, label='Gecappte Werte', zorder=5)\n",
    "    \n",
    "    axes[i].set_title(counter, fontsize=10)\n",
    "    axes[i].set_xlabel('Zeit')\n",
    "    axes[i].set_ylabel('Anzahl Fahrräder')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_literacy_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
