{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0060b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests_cache\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.preprocessing_utils import detect_intervals_with_missing_data, interpolate_short_gaps, remove_long_zero_intervals, split_long_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e36037",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec47c5",
   "metadata": {},
   "source": [
    "Bike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6511987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in .csv file of all bike data if it exists, otherwise download data from URLs and save as .csv\n",
    "\n",
    "file_path = \"../data/full_bike_data.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    #print(f\"Datei existiert: {file_path}\")\n",
    "    bike_data_raw = pd.read_csv(\"../data/full_bike_data.csv\", low_memory=False)\n",
    "    \n",
    "else:\n",
    "    #print(f\"Datei fehlt: {file_path}\")\n",
    "    url_list = []\n",
    "\n",
    "    # URLs zu Daten generieren nach folgendem Schema:\n",
    "    # https://mobidata-bw.de/daten/eco-counter/v2/fahrradzaehler_stundenwerten_{yyyymm}.csv.gz \n",
    "\n",
    "    for year in range(2013, 2026):\n",
    "        for month in range(1, 13):\n",
    "            yyyymm = f\"{year}{month:02d}\"\n",
    "            url = f\"https://mobidata-bw.de/daten/eco-counter/v2/fahrradzaehler_stundenwerten_{yyyymm}.csv.gz\"\n",
    "            \n",
    "            # Überprüfen, ob die URL existiert\n",
    "            response = requests_cache.CachedSession().head(url)\n",
    "            if response.status_code == 200:    \n",
    "                # in url_list hinzufügen\n",
    "                url_list.append(url)\n",
    "\n",
    "    general_columns = pd.read_csv(url_list[1]).columns.tolist()\n",
    "\n",
    "    # Erstelle CSV-Datei, in der Daten aller URLs gespeichert werden\n",
    "    full_bike_data = pd.DataFrame()\n",
    "    for url in url_list:\n",
    "        csv_data = pd.read_csv(url, low_memory=False)\n",
    "        assert list(csv_data.columns) == general_columns, f\"Spalten stimmen nicht überein in {url}\"\n",
    "        full_bike_data = pd.concat([full_bike_data, csv_data], ignore_index=True)\n",
    "\n",
    "    # Speichere full_bike_data lokal als CSV-Datei\n",
    "    full_bike_data.to_csv(\"../data/full_bike_data.csv\", index=False)   \n",
    "    bike_data_raw = full_bike_data.copy()\n",
    "\n",
    "bike_data_raw['iso_timestamp'] = pd.to_datetime(bike_data_raw['iso_timestamp'], utc = True, errors='coerce') # Isotimestamp ist lokale Zeit und berücksichtigt Sommerzeit\n",
    "bike_data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c20ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13319ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übersicht über die Counters in einer Stadt\n",
    "city = \"Stadt Heidelberg\"\n",
    "data_city = bike_data_raw[bike_data_raw['domain_name'] == city]\n",
    "\n",
    "counters = data_city[['counter_site', 'counter_site_id', 'counter_serial']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tracking = data_city.groupby(['counter_site', 'counter_site_id', 'counter_serial'], dropna = False)['iso_timestamp'] \\\n",
    "    .agg(first_timestamp='min', last_timestamp='max') \\\n",
    "    .reset_index()\n",
    "counters_with_tracking = counters.merge(tracking, on=['counter_site', 'counter_site_id', 'counter_serial'])\n",
    "\n",
    "counters_with_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e312946",
   "metadata": {},
   "source": [
    "Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file\n",
    "weather_data = pd.read_csv(\"../data/weather_per_city.csv\")\n",
    "weather_data['timestamp'] = pd.to_datetime(weather_data['date'], utc = True, errors='coerce') # Timestamp is in UTC\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1141649",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb956ea2",
   "metadata": {},
   "source": [
    "##### Clean Bike Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean data\n",
    "data_cleaned = bike_data_raw.copy()\n",
    "\n",
    "# Keep only relevant columns\n",
    "data_cleaned = data_cleaned[['domain_name', 'counter_site', 'longitude', 'latitude',\n",
    "       'iso_timestamp', 'channels_all']]\n",
    "\n",
    "# Only keep the following cities: Freiburg, Tübingen, Stuttgart, Ludwigsburg, Mannheim, Heidelberg, Reutlingen\n",
    "cities_to_keep = [\"Stadt Freiburg\", \"Stadt Tübingen\", \"Landeshauptstadt Stuttgart\", \n",
    "                  \"Stadt Ludwigsburg\", \"Stadt Mannheim\", \"Stadt Heidelberg\", \"Stadt Reutlingen\"]\n",
    "data_cleaned = data_cleaned[data_cleaned['domain_name'].isin(cities_to_keep)].copy()\n",
    "\n",
    "# 'Isotimestamp' is local time and considers 'Sommerzeit'. Therefore, we use this for better accuracy in time representation.\n",
    "# Exchange 'timestamp' with 'iso_timestamp' and convert to datetime with UTC timezone.\n",
    "# Drop 'timezone' as this is identical for all entries.\n",
    "data_cleaned['timestamp'] = pd.to_datetime(data_cleaned['iso_timestamp'], utc = True, errors='coerce') \n",
    "data_cleaned['timestamp'] = data_cleaned['timestamp'].dt.tz_convert('Europe/Berlin')\n",
    "data_cleaned = data_cleaned.drop(columns=['iso_timestamp'])\n",
    "\n",
    "# Rename columns for better clarity\n",
    "data_cleaned = data_cleaned.rename(columns={'domain_name': 'city', 'channels_all': 'count'})\n",
    "\n",
    "# Save cleaned data\n",
    "data_cleaned.to_csv(\"../data/full_bike_data_cleaned.csv\", index=False)\n",
    "\n",
    "bike_data = data_cleaned.copy()\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb08f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac002e",
   "metadata": {},
   "source": [
    "Sanity Check: Do channels_in & channels_out & channel_unknown sum up to channels out? # TODO: Add other sanity checks we did like checking that # counter_name = # counter ids etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checksum = bike_data_raw.copy()\n",
    "# ersetze 'na' Werte in den 3 channel spalten durch 0, um Checksum berechnen zu können\n",
    "data_checksum['channels_in'] = np.where(data_checksum['channels_in'].eq('na'), 0, data_checksum['channels_in'])\n",
    "data_checksum['channels_out'] = np.where(data_checksum['channels_out'].eq('na'), int(0), data_checksum['channels_out'])\n",
    "data_checksum['channels_unknown'] = np.where(data_checksum['channels_unknown'].eq('na'), int(0), data_checksum['channels_unknown'])\n",
    "\n",
    "# konvertiere die 4 Spalten in Integer\n",
    "data_checksum[['channels_in', 'channels_out', 'channels_unknown', 'count']] = data_checksum[['channels_in', 'channels_out', 'channels_unknown', 'count']].astype(int)\n",
    "sum_cols = ['channels_in', 'channels_out', 'channels_unknown']\n",
    "\n",
    "# Prüfe, ob die Summe der 3 channel Spalten der count Spalte entspricht\n",
    "data_checksum['checksum_correct'] = data_checksum[sum_cols].sum(axis=1).eq(data_checksum['count'])\n",
    "print(data_checksum['checksum_correct'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bf509",
   "metadata": {},
   "source": [
    "##### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46274d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv. file with full data\n",
    "bike_data = pd.read_csv(\"../data/full_bike_data_cleaned.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs in 'count' column\n",
    "nan_counts = bike_data['count'].isna().sum()\n",
    "print(f'Found {nan_counts} NaN values in count column.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ec101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing city: Stadt Freiburg\n",
      "================================================================================\n",
      "Processing counter: Wiwilibrücke\n",
      "Processing counter: FR2 Güterbahn Süd / Ferd.-Weiß-Str.\n",
      "Processing counter: FR1 Dreisam / Otto-Wels-Str.\n",
      "\n",
      "Processing city: Landeshauptstadt Stuttgart\n",
      "================================================================================\n",
      "Processing counter: König-Karls-Brücke Barometer\n",
      "Processing counter: Böblinger Straße\n",
      "Processing counter: Taubenheimstraße\n",
      "Processing counter: Waiblinger Straße\n",
      "Processing counter: Samaraweg\n",
      "Processing counter: Waldburgstraße\n",
      "Processing counter: Lautenschlager Straße\n",
      "Processing counter: Tübinger Straße\n",
      "Processing counter: Inselstraße\n",
      "Processing counter: Kremmlerstraße\n",
      "Processing counter: Kirchheimer Straße\n",
      "Processing counter: Stuttgarter Straße\n",
      "Processing counter: Solitudestraße\n",
      "Processing counter: Am Kräherwald\n",
      "Processing counter: Neckartalstraße\n",
      "\n",
      "Processing city: Stadt Tübingen\n",
      "================================================================================\n",
      "Processing counter: Unterführung Steinlach/Karlstraße Südseite - Steinlachallee\n",
      "Processing counter: Fuß- & Radtunnel Südportal - Derendinger Allee\n",
      "Processing counter: Neckartalradweg Hirschau - parallel L371\n",
      "Processing counter: Radbrücke Mitte - Wöhrdstraße\n",
      "Processing counter: Radbrücke Ost\n",
      "\n",
      "Processing city: Stadt Mannheim\n",
      "================================================================================\n",
      "Processing counter: Renzstraße\n",
      "Processing counter: Kurpfalzbrücke\n",
      "Processing counter: Jungbuschbrücke\n",
      "Processing counter: Konrad-Adenauer-Brücke\n",
      "Processing counter: Lindenhofüberführung\n",
      "Processing counter: Neckarauer Übergang -Schwetzinger Str.\n",
      "Processing counter: Schlosspark Lindenhof (Richtung Jugendherberge)\n",
      "Processing counter: Feudenheimstr. stadtauswärts\n",
      "Processing counter: Luzenbergstr.\n",
      "Processing counter: Feudenheimerstr. stadteinwärts\n",
      "Processing counter: B38. RI. AUS\n",
      "Processing counter: Theodor-Heuss-Anlage. RI. IN.\n",
      "Processing counter: Theodor-Heuss-Anlage. RI. AUS\n",
      "Processing counter: Fernmeldeturm.\n",
      "\n",
      "Processing city: Stadt Heidelberg\n",
      "================================================================================\n",
      "Processing counter: Ernst-Walz-Brücke Querschnitt\n",
      "Processing counter: Ernst-Walz-Brücke West - alt\n",
      "Processing counter: Plöck\n",
      "Processing counter: Gaisbergstraße\n",
      "Processing counter: Mannheimer Straße\n",
      "Processing counter: Thedor-Heuss-Brücke Querschnitt\n",
      "Processing counter: Rohrbacher Straße Querschnitt\n",
      "Processing counter: Liebermannstraße\n",
      "Processing counter: Schlierbacher Landstraße\n",
      "Processing counter: Ziegelhäuser Landstraße\n",
      "Processing counter: Kurfürstenanlage Querschnitt\n",
      "Processing counter: Hardtstraße\n",
      "Processing counter: Bahnstadtpromenade\n",
      "Processing counter: Berliner Straße Querschnitt\n",
      "Processing counter: Eppelheimer Str. Querschnitt\n",
      "\n",
      "Processing city: Stadt Ludwigsburg\n",
      "================================================================================\n",
      "Processing counter: Marbacher Straße - Favoritepark\n",
      "Processing counter: Alleenstraße\n",
      "Processing counter: Marbacher Straße - Neckarbrücke\n",
      "Processing counter: Schlieffenstraße\n",
      "Processing counter: Fuchshof\n",
      "Processing counter: Seestraße\n",
      "Processing counter: Schlossstraße\n",
      "Processing counter: Kesseläcker (Verl. Nussackerweg)\n",
      "Processing counter: Zugwiesen\n",
      "Processing counter: Solitudeallee\n",
      "Processing counter: Aldinger Straße\n",
      "Processing counter: Bottwartalstraße\n",
      "Processing counter: Bismarckstraße\n",
      "Processing counter: Königinallee\n",
      "Processing counter: Friedrich-Ebert-Straße\n",
      "\n",
      "Processing city: Stadt Reutlingen\n",
      "================================================================================\n",
      "Processing counter: Tübinger Tor\n",
      "Processing counter: Charlottenstraße\n",
      "Processing counter: Konrad-Adenauer-Straße\n",
      "Processing counter: Metzgerstraße\n",
      "Processing counter: Bellinostraße\n",
      "Processing counter: Hindenburgstraße\n",
      "Processing counter: Moltkestraße\n",
      "Processing counter: Unter den Linden\n",
      "\n",
      "Counters remaining after applying MIN_YEARS filter: 77\n",
      "\n",
      "Preprocessing complete.\n",
      "Final dataset shape: (3678215, 6)\n",
      "\n",
      "Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>counter_site</th>\n",
       "      <th>total_missing</th>\n",
       "      <th>interpolated</th>\n",
       "      <th>removed zeros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stadt Freiburg</td>\n",
       "      <td>Wiwilibrücke</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stadt Freiburg</td>\n",
       "      <td>FR2 Güterbahn Süd / Ferd.-Weiß-Str.</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stadt Freiburg</td>\n",
       "      <td>FR1 Dreisam / Otto-Wels-Str.</td>\n",
       "      <td>6306</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stadt Freiburg</td>\n",
       "      <td>FR1 Dreisam / Otto-Wels-Str._2</td>\n",
       "      <td>6306</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stadt Freiburg</td>\n",
       "      <td>FR1 Dreisam / Otto-Wels-Str._3</td>\n",
       "      <td>6306</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Stadt Reutlingen</td>\n",
       "      <td>Metzgerstraße</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Stadt Reutlingen</td>\n",
       "      <td>Bellinostraße</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Stadt Reutlingen</td>\n",
       "      <td>Hindenburgstraße</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Stadt Reutlingen</td>\n",
       "      <td>Moltkestraße</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Stadt Reutlingen</td>\n",
       "      <td>Unter den Linden</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 city                         counter_site  total_missing  \\\n",
       "0      Stadt Freiburg                         Wiwilibrücke             13   \n",
       "1      Stadt Freiburg  FR2 Güterbahn Süd / Ferd.-Weiß-Str.             12   \n",
       "2      Stadt Freiburg         FR1 Dreisam / Otto-Wels-Str.           6306   \n",
       "3      Stadt Freiburg       FR1 Dreisam / Otto-Wels-Str._2           6306   \n",
       "4      Stadt Freiburg       FR1 Dreisam / Otto-Wels-Str._3           6306   \n",
       "..                ...                                  ...            ...   \n",
       "324  Stadt Reutlingen                        Metzgerstraße              5   \n",
       "325  Stadt Reutlingen                        Bellinostraße              5   \n",
       "326  Stadt Reutlingen                     Hindenburgstraße              5   \n",
       "327  Stadt Reutlingen                         Moltkestraße              5   \n",
       "328  Stadt Reutlingen                     Unter den Linden              5   \n",
       "\n",
       "     interpolated  removed zeros  \n",
       "0              13              0  \n",
       "1              12              0  \n",
       "2              12              0  \n",
       "3              12              0  \n",
       "4              12              0  \n",
       "..            ...            ...  \n",
       "324             5              0  \n",
       "325             5              0  \n",
       "326             5              0  \n",
       "327             5              0  \n",
       "328             5              0  \n",
       "\n",
       "[329 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LONG_ZERO_LIMIT = 168      # 1 week\n",
    "INTERPOLATION_LIMIT = 3   # hours\n",
    "MIN_TS = 2 * 365 * 24     # Minimum years of data required per counter (2 years)\n",
    "\n",
    "all_counters_processed = []\n",
    "summary_list = []\n",
    "\n",
    "for city in bike_data['city'].unique():\n",
    "    print(f\"\\nProcessing city: {city}\\n{'='*80}\")\n",
    "    df_city = bike_data[bike_data['city'] == city]\n",
    "\n",
    "    for counter in df_city['counter_site'].unique():\n",
    "        print(f\"Processing counter: {counter}\")\n",
    "\n",
    "        df_counter = df_city[\n",
    "            df_city['counter_site'] == counter\n",
    "        ].copy()\n",
    "\n",
    "        df_counter['timestamp'] = pd.to_datetime(\n",
    "            df_counter['timestamp'], utc=True\n",
    "        )\n",
    "        df_counter['count'] = pd.to_numeric(\n",
    "            df_counter['count'], errors='coerce'\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Remove long zero-count intervals\n",
    "        # -------------------------------------------------\n",
    "        df_counter, removed_count = remove_long_zero_intervals(\n",
    "            df_counter, LONG_ZERO_LIMIT\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Reindex to hourly frequency\n",
    "        # -------------------------------------------------\n",
    "        df_counter = df_counter.set_index('timestamp').sort_index()\n",
    "\n",
    "        full_index = pd.date_range(\n",
    "            df_counter.index.min(),\n",
    "            df_counter.index.max(),\n",
    "            freq='H',\n",
    "            tz='UTC'\n",
    "        )\n",
    "\n",
    "        df_counter = df_counter.reindex(full_index)\n",
    "        df_counter.index.name = 'timestamp'\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Detect missing intervals\n",
    "        # -------------------------------------------------\n",
    "        missing_intervals = detect_intervals_with_missing_data(\n",
    "            df_counter,\n",
    "            column='count',\n",
    "            mode='missing'\n",
    "        )\n",
    "\n",
    "        total_missing = missing_intervals['n_points'].sum()\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Interpolate short gaps\n",
    "        # -------------------------------------------------\n",
    "        df_counter, interpolated_count = interpolate_short_gaps(\n",
    "            df_counter,\n",
    "            missing_intervals,\n",
    "            INTERPOLATION_LIMIT\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Fill metadata\n",
    "        # -------------------------------------------------\n",
    "        meta_cols = ['city', 'counter_site', 'longitude', 'latitude']\n",
    "        df_counter[meta_cols] = df_counter[meta_cols].ffill().bfill()\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Split long gaps\n",
    "        # -------------------------------------------------\n",
    "        long_gaps = missing_intervals[\n",
    "            missing_intervals['n_points'] > INTERPOLATION_LIMIT\n",
    "        ]\n",
    "\n",
    "        df_counter = split_long_gaps(df_counter, long_gaps)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Final validation + collection\n",
    "        # -------------------------------------------------\n",
    "        for site, g in df_counter.groupby('counter_site'):\n",
    "            if g['count'].isna().any():\n",
    "                raise ValueError(\n",
    "                    f\"NaNs remain after processing counter {site}\"\n",
    "                )\n",
    "\n",
    "            all_counters_processed.append(g)\n",
    "            summary_list.append({\n",
    "                'city': city,\n",
    "                'counter_site': site,\n",
    "                'total_missing': total_missing,\n",
    "                'interpolated': interpolated_count,\n",
    "                'removed zeros': removed_count\n",
    "            })\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Final outputs\n",
    "# =====================================================\n",
    "\n",
    "bike_data_final = (\n",
    "    pd.concat(all_counters_processed)\n",
    "      .sort_index()\n",
    "      .reset_index()\n",
    "      .rename(columns={'index': 'timestamp'})\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# =====================================================\n",
    "# Remove counters with less than 2 years of data\n",
    "# =====================================================\n",
    "counter_data_counts = bike_data_final.groupby('counter_site')['count'].count()\n",
    "counters_to_keep = counter_data_counts[counter_data_counts >= MIN_TS].index\n",
    "bike_data_final = bike_data_final[bike_data_final['counter_site'].isin(counters_to_keep)]\n",
    "\n",
    "summary_df = summary_df[summary_df['counter_site'].isin(counters_to_keep)]\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(f\"Final dataset shape: {bike_data_final.shape}\")\n",
    "print(f\"\\nCounters remaining: {bike_data_final['counter_site'].nunique()}\")\n",
    "print(\"\\nSummary:\")\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_literacy_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
