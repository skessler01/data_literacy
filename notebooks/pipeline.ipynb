{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0060b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests_cache\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.preprocessing_utils import detect_intervals_with_missing_data, interpolate_short_gaps, remove_long_zero_intervals, split_long_gaps, cap_outliers_by_time_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e36037",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec47c5",
   "metadata": {},
   "source": [
    "Bike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6511987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operator_name</th>\n",
       "      <th>domain_name</th>\n",
       "      <th>domain_id</th>\n",
       "      <th>counter_site</th>\n",
       "      <th>counter_site_id</th>\n",
       "      <th>counter_serial</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>timezone</th>\n",
       "      <th>iso_timestamp</th>\n",
       "      <th>channels_in</th>\n",
       "      <th>channels_out</th>\n",
       "      <th>channels_unknown</th>\n",
       "      <th>channels_all</th>\n",
       "      <th>site_temperature</th>\n",
       "      <th>site_rain_accumulation</th>\n",
       "      <th>site_snow_accumulation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eco Counter GmbH</td>\n",
       "      <td>Stadt Karlsruhe</td>\n",
       "      <td>752</td>\n",
       "      <td>Erbprinzenstraße</td>\n",
       "      <td>100004165</td>\n",
       "      <td>Y2H16070301</td>\n",
       "      <td>8.402715</td>\n",
       "      <td>49.007286</td>\n",
       "      <td>(UTC+01:00) Europe/Paris DST</td>\n",
       "      <td>2012-12-31 23:00:00+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>na</td>\n",
       "      <td>19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eco Counter GmbH</td>\n",
       "      <td>Stadt Karlsruhe</td>\n",
       "      <td>752</td>\n",
       "      <td>Erbprinzenstraße</td>\n",
       "      <td>100004165</td>\n",
       "      <td>Y2H16070301</td>\n",
       "      <td>8.402715</td>\n",
       "      <td>49.007286</td>\n",
       "      <td>(UTC+01:00) Europe/Paris DST</td>\n",
       "      <td>2013-01-01 00:00:00+00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>na</td>\n",
       "      <td>33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eco Counter GmbH</td>\n",
       "      <td>Stadt Karlsruhe</td>\n",
       "      <td>752</td>\n",
       "      <td>Erbprinzenstraße</td>\n",
       "      <td>100004165</td>\n",
       "      <td>Y2H16070301</td>\n",
       "      <td>8.402715</td>\n",
       "      <td>49.007286</td>\n",
       "      <td>(UTC+01:00) Europe/Paris DST</td>\n",
       "      <td>2013-01-01 01:00:00+00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>na</td>\n",
       "      <td>31</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eco Counter GmbH</td>\n",
       "      <td>Stadt Karlsruhe</td>\n",
       "      <td>752</td>\n",
       "      <td>Erbprinzenstraße</td>\n",
       "      <td>100004165</td>\n",
       "      <td>Y2H16070301</td>\n",
       "      <td>8.402715</td>\n",
       "      <td>49.007286</td>\n",
       "      <td>(UTC+01:00) Europe/Paris DST</td>\n",
       "      <td>2013-01-01 02:00:00+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>na</td>\n",
       "      <td>40</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eco Counter GmbH</td>\n",
       "      <td>Stadt Karlsruhe</td>\n",
       "      <td>752</td>\n",
       "      <td>Erbprinzenstraße</td>\n",
       "      <td>100004165</td>\n",
       "      <td>Y2H16070301</td>\n",
       "      <td>8.402715</td>\n",
       "      <td>49.007286</td>\n",
       "      <td>(UTC+01:00) Europe/Paris DST</td>\n",
       "      <td>2013-01-01 03:00:00+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>na</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      operator_name      domain_name  domain_id      counter_site  \\\n",
       "0  Eco Counter GmbH  Stadt Karlsruhe        752  Erbprinzenstraße   \n",
       "1  Eco Counter GmbH  Stadt Karlsruhe        752  Erbprinzenstraße   \n",
       "2  Eco Counter GmbH  Stadt Karlsruhe        752  Erbprinzenstraße   \n",
       "3  Eco Counter GmbH  Stadt Karlsruhe        752  Erbprinzenstraße   \n",
       "4  Eco Counter GmbH  Stadt Karlsruhe        752  Erbprinzenstraße   \n",
       "\n",
       "   counter_site_id counter_serial  longitude   latitude  \\\n",
       "0        100004165    Y2H16070301   8.402715  49.007286   \n",
       "1        100004165    Y2H16070301   8.402715  49.007286   \n",
       "2        100004165    Y2H16070301   8.402715  49.007286   \n",
       "3        100004165    Y2H16070301   8.402715  49.007286   \n",
       "4        100004165    Y2H16070301   8.402715  49.007286   \n",
       "\n",
       "                       timezone             iso_timestamp channels_in  \\\n",
       "0  (UTC+01:00) Europe/Paris DST 2012-12-31 23:00:00+00:00           9   \n",
       "1  (UTC+01:00) Europe/Paris DST 2013-01-01 00:00:00+00:00          15   \n",
       "2  (UTC+01:00) Europe/Paris DST 2013-01-01 01:00:00+00:00          17   \n",
       "3  (UTC+01:00) Europe/Paris DST 2013-01-01 02:00:00+00:00          14   \n",
       "4  (UTC+01:00) Europe/Paris DST 2013-01-01 03:00:00+00:00          13   \n",
       "\n",
       "  channels_out channels_unknown  channels_all site_temperature  \\\n",
       "0           10               na            19              5.0   \n",
       "1           18               na            33              5.0   \n",
       "2           14               na            31              5.0   \n",
       "3           26               na            40              5.0   \n",
       "4           17               na            30              5.0   \n",
       "\n",
       "  site_rain_accumulation site_snow_accumulation  \n",
       "0                    0.0                     na  \n",
       "1                    0.0                     na  \n",
       "2                    0.0                     na  \n",
       "3                    0.0                     na  \n",
       "4                    0.0                     na  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in .csv file of all bike data if it exists, otherwise download data from URLs and save as .csv\n",
    "\n",
    "file_path = \"../data/full_bike_data.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    #print(f\"Datei existiert: {file_path}\")\n",
    "    bike_data_raw = pd.read_csv(\"../data/full_bike_data.csv\", low_memory=False)\n",
    "    \n",
    "else:\n",
    "    #print(f\"Datei fehlt: {file_path}\")\n",
    "    url_list = []\n",
    "\n",
    "    # URLs zu Daten generieren nach folgendem Schema:\n",
    "    # https://mobidata-bw.de/daten/eco-counter/v2/fahrradzaehler_stundenwerten_{yyyymm}.csv.gz \n",
    "\n",
    "    for year in range(2013, 2026):\n",
    "        for month in range(1, 13):\n",
    "            yyyymm = f\"{year}{month:02d}\"\n",
    "            url = f\"https://mobidata-bw.de/daten/eco-counter/v2/fahrradzaehler_stundenwerten_{yyyymm}.csv.gz\"\n",
    "            \n",
    "            # Überprüfen, ob die URL existiert\n",
    "            response = requests_cache.CachedSession().head(url)\n",
    "            if response.status_code == 200:    \n",
    "                # in url_list hinzufügen\n",
    "                url_list.append(url)\n",
    "\n",
    "    general_columns = pd.read_csv(url_list[1]).columns.tolist()\n",
    "\n",
    "    # Erstelle CSV-Datei, in der Daten aller URLs gespeichert werden\n",
    "    full_bike_data = pd.DataFrame()\n",
    "    for url in url_list:\n",
    "        csv_data = pd.read_csv(url, low_memory=False)\n",
    "        assert list(csv_data.columns) == general_columns, f\"Spalten stimmen nicht überein in {url}\"\n",
    "        full_bike_data = pd.concat([full_bike_data, csv_data], ignore_index=True)\n",
    "\n",
    "    # Speichere full_bike_data lokal als CSV-Datei\n",
    "    full_bike_data.to_csv(\"../data/full_bike_data.csv\", index=False)   \n",
    "    bike_data_raw = full_bike_data.copy()\n",
    "\n",
    "bike_data_raw['iso_timestamp'] = pd.to_datetime(bike_data_raw['iso_timestamp'], utc = True, errors='coerce') # Isotimestamp ist lokale Zeit und berücksichtigt Sommerzeit\n",
    "bike_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5147be1",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c20ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6125443 entries, 0 to 6125442\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Dtype              \n",
      "---  ------                  -----              \n",
      " 0   operator_name           object             \n",
      " 1   domain_name             object             \n",
      " 2   domain_id               int64              \n",
      " 3   counter_site            object             \n",
      " 4   counter_site_id         int64              \n",
      " 5   counter_serial          object             \n",
      " 6   longitude               float64            \n",
      " 7   latitude                float64            \n",
      " 8   timezone                object             \n",
      " 9   iso_timestamp           datetime64[ns, UTC]\n",
      " 10  channels_in             object             \n",
      " 11  channels_out            object             \n",
      " 12  channels_unknown        object             \n",
      " 13  channels_all            int64              \n",
      " 14  site_temperature        object             \n",
      " 15  site_rain_accumulation  object             \n",
      " 16  site_snow_accumulation  object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(2), int64(3), object(11)\n",
      "memory usage: 794.5+ MB\n"
     ]
    }
   ],
   "source": [
    "bike_data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cities: 22\n",
      "['Stadt Karlsruhe' 'Stadt Freiburg' 'Landeshauptstadt Stuttgart'\n",
      " 'Stadt Tübingen' 'Stadt Lörrach' 'Stadt Heilbronn' 'Stadt Mannheim'\n",
      " 'Stadt Kirchheim Unter Teck' 'Stadt Heidelberg' 'Stadt Offenburg'\n",
      " 'Stadt Ludwigsburg' 'Stadt Konstanz' 'Landkreis Böblingen'\n",
      " 'Ravensburg Tws Gmbh & Co. Kg' 'Stadt Ulm' 'Stadtverwaltung Aalen'\n",
      " 'Stadt Reutlingen' 'Landratsamt Rems-Murr-Kreis' 'Stadt Singen'\n",
      " 'Stadt Bad Säckingen' 'Landratsamt Ostalbkreis'\n",
      " 'Regierungspräsidium Stuttgart Aussenstelle Heilbronn']\n"
     ]
    }
   ],
   "source": [
    "# Cities in the dataset\n",
    "unique_cities = bike_data_raw['domain_name'].unique()\n",
    "print(f\"Number of cities: {len(unique_cities)}\")\n",
    "print(unique_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8684596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of counters: 129\n"
     ]
    }
   ],
   "source": [
    "# Counter in the dataset\n",
    "unique_counters = bike_data_raw['counter_site_id'].unique()\n",
    "print(f\"Number of counters: {len(unique_counters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9896d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time range: 2012-12-31 23:00:00+00:00 to 2025-12-31 22:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Time range of the dataset\n",
    "min_timestamp = bike_data_raw['iso_timestamp'].min()\n",
    "max_timestamp = bike_data_raw['iso_timestamp'].max()\n",
    "print(f\"Time range: {min_timestamp} to {max_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13319ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übersicht über die Counters in einer Stadt\n",
    "city = \"Stadt Heidelberg\"\n",
    "data_city = bike_data_raw[bike_data_raw['domain_name'] == city]\n",
    "\n",
    "counters = data_city[['counter_site', 'counter_site_id', 'counter_serial']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tracking = data_city.groupby(['counter_site', 'counter_site_id', 'counter_serial'], dropna = False)['iso_timestamp'] \\\n",
    "    .agg(first_timestamp='min', last_timestamp='max') \\\n",
    "    .reset_index()\n",
    "counters_with_tracking = counters.merge(tracking, on=['counter_site', 'counter_site_id', 'counter_serial'])\n",
    "\n",
    "counters_with_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e312946",
   "metadata": {},
   "source": [
    "Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file\n",
    "weather_data = pd.read_csv(\"../data/weather_per_city.csv\")\n",
    "weather_data['timestamp'] = pd.to_datetime(weather_data['date'], utc = True, errors='coerce').dt.tz_convert('Europe/Berlin') # Timestamp is in UTC\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1141649",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb956ea2",
   "metadata": {},
   "source": [
    "##### Clean Bike Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean data\n",
    "data_cleaned = bike_data_raw.copy()\n",
    "\n",
    "# Keep only relevant columns\n",
    "data_cleaned = data_cleaned[['domain_name', 'counter_site', 'longitude', 'latitude',\n",
    "       'iso_timestamp', 'channels_all']]\n",
    "\n",
    "# Only keep the following cities: Freiburg, Tübingen, Stuttgart, Ludwigsburg, Mannheim, Heidelberg, Reutlingen\n",
    "cities_to_keep = [\"Stadt Freiburg\", \"Stadt Tübingen\", \"Landeshauptstadt Stuttgart\", \n",
    "                  \"Stadt Ludwigsburg\", \"Stadt Mannheim\", \"Stadt Heidelberg\", \"Stadt Reutlingen\"]\n",
    "data_cleaned = data_cleaned[data_cleaned['domain_name'].isin(cities_to_keep)].copy()\n",
    "\n",
    "# Rename columns for better clarity\n",
    "data_cleaned = data_cleaned.rename(columns={'domain_name': 'city', 'channels_all': 'count', 'iso_timestamp': 'timestamp'})\n",
    "\n",
    "# Convert timestamp to Europe/Berlin timezone\n",
    "data_cleaned['timestamp'] = pd.to_datetime(data_cleaned['timestamp'], utc = True, errors='coerce').dt.tz_convert('Europe/Berlin')\n",
    "\n",
    "# Save cleaned data\n",
    "data_cleaned.to_csv(\"../data/full_bike_data_cleaned.csv\", index=False)\n",
    "\n",
    "bike_data = data_cleaned.copy()\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb08f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac002e",
   "metadata": {},
   "source": [
    "Sanity Check: Do channels_in & channels_out & channel_unknown sum up to channels out? # TODO: Add other sanity checks we did like checking that # counter_name = # counter ids etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_checksum = bike_data_raw.copy()\n",
    "# ersetze 'na' Werte in den 3 channel spalten durch 0, um Checksum berechnen zu können\n",
    "data_checksum['channels_in'] = np.where(data_checksum['channels_in'].eq('na'), 0, data_checksum['channels_in'])\n",
    "data_checksum['channels_out'] = np.where(data_checksum['channels_out'].eq('na'), int(0), data_checksum['channels_out'])\n",
    "data_checksum['channels_unknown'] = np.where(data_checksum['channels_unknown'].eq('na'), int(0), data_checksum['channels_unknown'])\n",
    "\n",
    "# konvertiere die 4 Spalten in Integer\n",
    "data_checksum[['channels_in', 'channels_out', 'channels_unknown', 'count']] = data_checksum[['channels_in', 'channels_out', 'channels_unknown', 'count']].astype(int)\n",
    "sum_cols = ['channels_in', 'channels_out', 'channels_unknown']\n",
    "\n",
    "# Prüfe, ob die Summe der 3 channel Spalten der count Spalte entspricht\n",
    "data_checksum['checksum_correct'] = data_checksum[sum_cols].sum(axis=1).eq(data_checksum['count'])\n",
    "print(data_checksum['checksum_correct'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408e6bf",
   "metadata": {},
   "source": [
    "##### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24418699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv. file with full data\n",
    "bike_data = pd.read_csv(\"../data/full_bike_data_cleaned.csv\", low_memory=False)\n",
    "bike_data['timestamp'] = pd.to_datetime(bike_data['timestamp'], utc=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap high counter values based on hour-of-day and day-of-year patterns\n",
    "cap_const = 10\n",
    "bike_data_capped = cap_outliers_by_time_pattern(bike_data, \"count\", cap_const=cap_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bf509",
   "metadata": {},
   "source": [
    "##### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs in 'count' column\n",
    "nan_counts = bike_data_capped['count'].isna().sum()\n",
    "print(f'Found {nan_counts} NaN values in count column.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ec101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LONG_ZERO_LIMIT = 168      # 1 week\n",
    "INTERPOLATION_LIMIT = 3   # hours\n",
    "MIN_TS = 2 * 365 * 24     # Minimum years of data required per counter (2 years)\n",
    "\n",
    "all_counters_processed = []\n",
    "summary_list = []\n",
    "\n",
    "for city in bike_data_capped['city'].unique():\n",
    "    print(f\"\\nProcessing city: {city}\\n{'='*80}\")\n",
    "    df_city = bike_data_capped[bike_data_capped['city'] == city]\n",
    "\n",
    "    for counter in df_city['counter_site'].unique():\n",
    "        print(f\"Processing counter: {counter}\")\n",
    "\n",
    "        df_counter = df_city[\n",
    "            df_city['counter_site'] == counter\n",
    "        ].copy()\n",
    "\n",
    "        df_counter['timestamp'] = pd.to_datetime(\n",
    "            df_counter['timestamp'], utc=True\n",
    "        )\n",
    "        df_counter['count'] = pd.to_numeric(\n",
    "            df_counter['count'], errors='coerce'\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Remove long zero-count intervals\n",
    "        # -------------------------------------------------\n",
    "        df_counter, removed_count = remove_long_zero_intervals(\n",
    "            df_counter, LONG_ZERO_LIMIT\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Reindex to hourly frequency\n",
    "        # -------------------------------------------------\n",
    "        df_counter = df_counter.set_index('timestamp').sort_index()\n",
    "\n",
    "        full_index = pd.date_range(\n",
    "            df_counter.index.min(),\n",
    "            df_counter.index.max(),\n",
    "            freq='H',\n",
    "            tz='UTC'\n",
    "        )\n",
    "\n",
    "        df_counter = df_counter.reindex(full_index)\n",
    "        df_counter.index.name = 'timestamp'\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Detect missing intervals\n",
    "        # -------------------------------------------------\n",
    "        missing_intervals = detect_intervals_with_missing_data(\n",
    "            df_counter,\n",
    "            column='count',\n",
    "            mode='missing'\n",
    "        )\n",
    "\n",
    "        total_missing = missing_intervals['n_points'].sum()\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Interpolate short gaps\n",
    "        # -------------------------------------------------\n",
    "        df_counter, interpolated_count = interpolate_short_gaps(\n",
    "            df_counter,\n",
    "            missing_intervals,\n",
    "            INTERPOLATION_LIMIT\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Fill metadata\n",
    "        # -------------------------------------------------\n",
    "        meta_cols = ['city', 'counter_site', 'longitude', 'latitude']\n",
    "        df_counter[meta_cols] = df_counter[meta_cols].ffill().bfill()\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Split long gaps\n",
    "        # -------------------------------------------------\n",
    "        long_gaps = missing_intervals[\n",
    "            missing_intervals['n_points'] > INTERPOLATION_LIMIT\n",
    "        ]\n",
    "\n",
    "        df_counter = split_long_gaps(df_counter, long_gaps)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Final validation + collection\n",
    "        # -------------------------------------------------\n",
    "        for site, g in df_counter.groupby('counter_site'):\n",
    "            if g['count'].isna().any():\n",
    "                raise ValueError(\n",
    "                    f\"NaNs remain after processing counter {site}\"\n",
    "                )\n",
    "\n",
    "            all_counters_processed.append(g)\n",
    "            summary_list.append({\n",
    "                'city': city,\n",
    "                'counter_site': site,\n",
    "                'total_missing': total_missing,\n",
    "                'interpolated': interpolated_count,\n",
    "                'removed zeros': removed_count\n",
    "            })\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Final outputs\n",
    "# =====================================================\n",
    "\n",
    "bike_data_final = (\n",
    "    pd.concat(all_counters_processed)\n",
    "      .sort_index()\n",
    "      .reset_index()\n",
    "      .rename(columns={'index': 'timestamp'})\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# =====================================================\n",
    "# Remove counters with less than 2 years of data\n",
    "# =====================================================\n",
    "counter_data_counts = bike_data_final.groupby('counter_site')['count'].count()\n",
    "counters_to_keep = counter_data_counts[counter_data_counts >= MIN_TS].index\n",
    "bike_data_final = bike_data_final[bike_data_final['counter_site'].isin(counters_to_keep)]\n",
    "\n",
    "summary_df = summary_df[summary_df['counter_site'].isin(counters_to_keep)]\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(f\"Final dataset shape: {bike_data_final.shape}\")\n",
    "print(f\"\\nCounters remaining: {bike_data_final['counter_site'].nunique()}\")\n",
    "print(\"\\nSummary:\")\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final preprosessed data\n",
    "bike_data_final.to_csv(\"../data/full_bike_data_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2253b",
   "metadata": {},
   "source": [
    "##### plots- kann man wieder löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a77f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike_data_final einzelne Counter einer Stadt plotten   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "city = [\"Stadt Tübingen\", \"Stadt Freiburg\", \"Landeshauptstadt Stuttgart\"]\n",
    "\n",
    "# Filter für die ausgewählte Stadt\n",
    "city_data = bike_data_final[bike_data_final['city'].isin(city)].copy()\n",
    "\n",
    "# Konvertiere timestamp zu datetime falls nötig\n",
    "city_data['timestamp'] = pd.to_datetime(city_data['timestamp'])\n",
    "\n",
    "# Alle einzigartigen Counter\n",
    "counters = city_data['counter_site'].unique()\n",
    "n_counters = len(counters)\n",
    "\n",
    "# Erstelle Subplots\n",
    "n_cols = 1\n",
    "n_rows = n_counters\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten() if n_counters > 1 else [axes]\n",
    "\n",
    "# Plotte jeden Counter\n",
    "for i, counter in enumerate(counters):\n",
    "    counter_data = city_data[city_data['counter_site'] == counter]\n",
    "    \n",
    "    axes[i].plot(counter_data['timestamp'], counter_data['count'], linewidth=0.5)\n",
    "    axes[i].set_title(counter, fontsize=10)\n",
    "    axes[i].set_xlabel('Zeit')\n",
    "    axes[i].set_ylabel('Anzahl Fahrräder')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c623b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot capped data: for all counters of one city. \n",
    "# Mark the capped values in green. and the old the original value of the capped valuen in red\n",
    "\n",
    "# Filter für die ausgewählte Stadt\n",
    "city_data_original = bike_data[bike_data['city'].isin(city)].copy()\n",
    "city_data_capped = bike_data_capped[bike_data_capped['city'].isin(city)].copy()\n",
    "\n",
    "# Konvertiere timestamps zu datetime\n",
    "city_data_original['timestamp'] = pd.to_datetime(city_data_original['timestamp'])\n",
    "city_data_capped['timestamp'] = pd.to_datetime(city_data_capped['timestamp'])\n",
    "\n",
    "# Merge original und capped für Vergleich\n",
    "merged = city_data_original.merge(\n",
    "    city_data_capped[['timestamp', 'counter_site', 'count']], \n",
    "    on=['timestamp', 'counter_site'], \n",
    "    suffixes=('_original', '_capped'),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Alle einzigartigen Counter\n",
    "counters = merged['counter_site'].unique()\n",
    "n_counters = len(counters)\n",
    "\n",
    "# Erstelle Subplots\n",
    "n_cols = 1\n",
    "n_rows = n_counters\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten() if n_counters > 1 else [axes]\n",
    "\n",
    "# Plotte jeden Counter\n",
    "for i, counter in enumerate(counters):\n",
    "    counter_merged = merged[merged['counter_site'] == counter].sort_values('timestamp')\n",
    "    \n",
    "    # Plotte Original-Linie in blau\n",
    "    axes[i].plot(counter_merged['timestamp'], counter_merged['count_original'], \n",
    "                 linewidth=0.8, color='blue', alpha=0.8, label='Original')\n",
    "    \n",
    "    # Plotte gecappte Linie in schwarz\n",
    "    axes[i].plot(counter_merged['timestamp'], counter_merged['count_capped'], \n",
    "                 linewidth=0.8, color='black', alpha=0.6, label='Gecappt')\n",
    "    \n",
    "    # Markiere Punkte wo gecappt wurde (original > capped)\n",
    "    capped_mask = counter_merged['count_original'] > counter_merged['count_capped']\n",
    "    capped_points = counter_merged[capped_mask]\n",
    "    \n",
    "    # Rote Punkte für Original-Werte, die gecappt wurden\n",
    "    axes[i].scatter(capped_points['timestamp'], capped_points['count_original'], \n",
    "                   color='red', s=20, alpha=0.9, label='Original (gecappt)', zorder=5)\n",
    "    \n",
    "    # Grüne Punkte für die gecappten Werte\n",
    "    axes[i].scatter(capped_points['timestamp'], capped_points['count_capped'], \n",
    "                   color='green', s=20, alpha=0.9, label='Gecappte Werte', zorder=5)\n",
    "    \n",
    "    axes[i].set_title(counter, fontsize=10)\n",
    "    axes[i].set_xlabel('Zeit')\n",
    "    axes[i].set_ylabel('Anzahl Fahrräder')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_literacy_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
